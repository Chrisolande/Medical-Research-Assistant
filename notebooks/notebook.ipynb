{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "cellUniqueIdByVincent": "be98c"
   },
   "outputs": [],
   "source": [
    "from PubMedDownloader import PubMedEntrezDownloader\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "async def main():\n",
    "    downloader = PubMedEntrezDownloader(\"olandechris@gmail.com\")\n",
    "    pmids = await downloader.search_pubmed(\"\", max_results=10000)\n",
    "    articles = await downloader.fetch_article_details(pmids)\n",
    "    downloader.save_to_json(articles, \"results.json\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from document_processor import DocumentProcessor\n",
    "docs = DocumentProcessor()\n",
    "docs.get_stats(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import logging\n",
    "from batchprocessor import PMCBatchProcessor\n",
    "from document_processor import DocumentProcessor\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"MainScript\")\n",
    "\n",
    "async def main():\n",
    "    doc_processor = DocumentProcessor(embeddings_model=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "    batch_processor = PMCBatchProcessor(\n",
    "        document_processor=doc_processor,\n",
    "        batch_size=96,\n",
    "        max_concurrent_batches=3,\n",
    "        retry_attempts=2,\n",
    "        retry_delay=1.0,\n",
    "        inter_batch_delay=0.1\n",
    "    )\n",
    "\n",
    "    file_path = \"research20250605_002659.json\" \n",
    "    output_directory = \"output/processed_pmc_data\"\n",
    "\n",
    "    logger.info(f\"Starting batch processing of {file_path}\")\n",
    "\n",
    "    try:\n",
    "        processing_results = await batch_processor.process_pmc_file_async(\n",
    "            file_path=file_path\n",
    "        )\n",
    "\n",
    "        batch_processor.save_results(processing_results, output_directory, save_batch_details=True)\n",
    "\n",
    "        s = processing_results['processing_summary']\n",
    "        logger.info(f\"Processing complete: {s['total_documents']:,} docs → {s['total_chunks']:,} chunks ({s['processing_time']:.1f}s)\")\n",
    "        logger.info(f\"Success rate: {s['success_rate']:.1f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during batch processing: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from langchain.schema import Document\n",
    "\n",
    "data_path = Path(\"../output/processed_pmc_data/pmc_chunks.json\")\n",
    "with data_path.open(encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create documents\n",
    "documents = [\n",
    "    Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"])\n",
    "    for doc in data[\"documents\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlp.vectorstore import VectorStore\n",
    "\n",
    "vector_store = VectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# await vector_store._create_vector_index(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "llm = ChatOpenAI(model=\"meta-llama/llama-3.3-70b-instruct\",\n",
    "                                api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "                                openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "                                temperature=0,\n",
    "                                streaming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.knowledge_graph.knowledge_graph import KnowledgeGraph\n",
    "kg = KnowledgeGraph(\n",
    "    cache_dir = \"../my_cache\",\n",
    "    batch_size=100,  # Process 50 documents at a time with spaCy\n",
    "    # max_concurrent_llm_calls=10\n",
    ")\n",
    "\n",
    "# Build the knowledge graph\n",
    "graph = await kg.build_knowledge_graph(documents, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlp.rag_chain import QueryEngine\n",
    "engine = QueryEngine(vector_store = vector_store, knowledge_graph = kg, llm = llm)\n",
    "# vector_store.retrieve_relevant_documents(\"What is the effect of Gaza war on Children?\", filter_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are the effects of the Gaza war on children?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response, traversal_path, filtered_content = await engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.knowledge_graph.graph_viz import GraphVisualizer\n",
    "visualizer = GraphVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "await visualizer.visualize_traversal_async(graph, traversal_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "# from graph_viz import GraphVisualizer\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class NodeMetadata:\n",
    "    \"\"\"Enhanced metadata for each node.\"\"\"\n",
    "    processing_time_ms: float\n",
    "    confidence_score: float\n",
    "    error_rate: float\n",
    "    throughput_capacity: int\n",
    "    dependencies: List[int] = field(default_factory=list)\n",
    "    node_type: str = \"standard\"\n",
    "\n",
    "# 1. Create a comprehensive knowledge processing graph\n",
    "logger.info(\"Building complex knowledge processing graph...\")\n",
    "G = nx.Graph()\n",
    "\n",
    "# Define complex edge relationships with multiple attributes\n",
    "edges = [\n",
    "    # Primary knowledge flow path\n",
    "    (1, 2, {'weight': 0.85, 'flow_type': 'primary', 'latency_ms': 15, 'bandwidth': 100}),\n",
    "    (2, 3, {'weight': 0.72, 'flow_type': 'primary', 'latency_ms': 25, 'bandwidth': 95}),\n",
    "    (3, 4, {'weight': 0.91, 'flow_type': 'primary', 'latency_ms': 35, 'bandwidth': 90}),\n",
    "    (4, 8, {'weight': 0.68, 'flow_type': 'primary', 'latency_ms': 45, 'bandwidth': 85}),\n",
    "    \n",
    "    # Secondary processing paths\n",
    "    (1, 5, {'weight': 0.76, 'flow_type': 'secondary', 'latency_ms': 20, 'bandwidth': 80}),\n",
    "    (5, 6, {'weight': 0.58, 'flow_type': 'secondary', 'latency_ms': 30, 'bandwidth': 75}),\n",
    "    (6, 7, {'weight': 0.83, 'flow_type': 'secondary', 'latency_ms': 40, 'bandwidth': 70}),\n",
    "    (7, 8, {'weight': 0.94, 'flow_type': 'secondary', 'latency_ms': 50, 'bandwidth': 65}),\n",
    "    \n",
    "    # Cross-connections and feedback loops\n",
    "    (2, 5, {'weight': 0.64, 'flow_type': 'cross_connect', 'latency_ms': 12, 'bandwidth': 60}),\n",
    "    (3, 6, {'weight': 0.79, 'flow_type': 'cross_connect', 'latency_ms': 18, 'bandwidth': 55}),\n",
    "    (4, 7, {'weight': 0.66, 'flow_type': 'cross_connect', 'latency_ms': 22, 'bandwidth': 50}),\n",
    "    \n",
    "    # Specialized processing branches\n",
    "    (4, 9, {'weight': 0.43, 'flow_type': 'fallback', 'latency_ms': 60, 'bandwidth': 40}),\n",
    "    (8, 10, {'weight': 0.87, 'flow_type': 'primary', 'latency_ms': 28, 'bandwidth': 85}),\n",
    "    (10, 11, {'weight': 0.92, 'flow_type': 'primary', 'latency_ms': 32, 'bandwidth': 90}),\n",
    "    (11, 12, {'weight': 0.96, 'flow_type': 'primary', 'latency_ms': 38, 'bandwidth': 95}),\n",
    "    \n",
    "    # Alternative and backup paths\n",
    "    (9, 12, {'weight': 0.71, 'flow_type': 'fallback', 'latency_ms': 55, 'bandwidth': 45}),\n",
    "    (6, 10, {'weight': 0.54, 'flow_type': 'shortcut', 'latency_ms': 25, 'bandwidth': 35}),\n",
    "    (7, 11, {'weight': 0.61, 'flow_type': 'shortcut', 'latency_ms': 30, 'bandwidth': 40}),\n",
    "    \n",
    "    # Advanced processing nodes\n",
    "    (12, 13, {'weight': 0.88, 'flow_type': 'post_process', 'latency_ms': 20, 'bandwidth': 80}),\n",
    "    (13, 14, {'weight': 0.82, 'flow_type': 'post_process', 'latency_ms': 15, 'bandwidth': 75}),\n",
    "    (14, 15, {'weight': 0.95, 'flow_type': 'output', 'latency_ms': 10, 'bandwidth': 100}),\n",
    "    \n",
    "    # Quality assurance and validation\n",
    "    (11, 16, {'weight': 0.77, 'flow_type': 'validation', 'latency_ms': 35, 'bandwidth': 60}),\n",
    "    (16, 13, {'weight': 0.84, 'flow_type': 'validation', 'latency_ms': 25, 'bandwidth': 65}),\n",
    "    \n",
    "    # Feedback and learning loops\n",
    "    (15, 17, {'weight': 0.69, 'flow_type': 'feedback', 'latency_ms': 45, 'bandwidth': 30}),\n",
    "    (17, 1, {'weight': 0.33, 'flow_type': 'feedback', 'latency_ms': 80, 'bandwidth': 20}),\n",
    "]\n",
    "\n",
    "# Add all edges to graph\n",
    "for u, v, attrs in edges:\n",
    "    G.add_edge(u, v, **attrs)\n",
    "\n",
    "logger.info(f\"Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "# 2. Define comprehensive concept mappings with hierarchical structure\n",
    "concepts = {\n",
    "    1: {\n",
    "        'primary': [\"User Input Handler\", \"Request Router\", \"Entry Point\"],\n",
    "        'secondary': [\"Authentication\", \"Rate Limiting\", \"Input Validation\"],\n",
    "        'technical': [\"HTTP Parser\", \"JSON Decoder\", \"Request Sanitizer\"],\n",
    "        'metadata': NodeMetadata(15.2, 0.98, 0.02, 1000, [], \"input_handler\")\n",
    "    },\n",
    "    2: {\n",
    "        'primary': [\"Intent Classification\", \"NLU Engine\", \"Semantic Parser\"],\n",
    "        'secondary': [\"Language Detection\", \"Sentiment Analysis\", \"Toxicity Filter\"],\n",
    "        'technical': [\"BERT Embeddings\", \"Classification Head\", \"Attention Mechanism\"],\n",
    "        'metadata': NodeMetadata(45.8, 0.92, 0.08, 500, [1], \"ml_processor\")\n",
    "    },\n",
    "    3: {\n",
    "        'primary': [\"Named Entity Recognition\", \"Entity Linking\", \"Concept Extraction\"],\n",
    "        'secondary': [\"Coreference Resolution\", \"Temporal Extraction\", \"Relation Extraction\"],\n",
    "        'technical': [\"BiLSTM-CRF\", \"Knowledge Graph Lookup\", \"Fuzzy Matching\"],\n",
    "        'metadata': NodeMetadata(38.4, 0.89, 0.11, 400, [1, 2], \"nlp_processor\")\n",
    "    },\n",
    "    4: {\n",
    "        'primary': [\"Context Understanding\", \"Semantic Reasoning\", \"Knowledge Integration\"],\n",
    "        'secondary': [\"Inference Engine\", \"Logical Reasoning\", \"Contradiction Detection\"],\n",
    "        'technical': [\"Graph Neural Networks\", \"Transformer Architecture\", \"Attention Pooling\"],\n",
    "        'metadata': NodeMetadata(125.6, 0.85, 0.15, 200, [2, 3], \"reasoning_engine\")\n",
    "    },\n",
    "    5: {\n",
    "        'primary': [\"Query Expansion\", \"Synonym Generation\", \"Concept Broadening\"],\n",
    "        'secondary': [\"Word Sense Disambiguation\", \"Morphological Analysis\", \"Lemmatization\"],\n",
    "        'technical': [\"Word2Vec\", \"GloVe Embeddings\", \"Semantic Similarity\"],\n",
    "        'metadata': NodeMetadata(28.9, 0.94, 0.06, 600, [1], \"text_processor\")\n",
    "    },\n",
    "    6: {\n",
    "        'primary': [\"Context Aggregation\", \"Multi-source Fusion\", \"Information Synthesis\"],\n",
    "        'secondary': [\"Conflict Resolution\", \"Source Weighting\", \"Confidence Scoring\"],\n",
    "        'technical': [\"Ensemble Methods\", \"Weighted Voting\", \"Bayesian Fusion\"],\n",
    "        'metadata': NodeMetadata(67.3, 0.91, 0.09, 300, [3, 5], \"aggregator\")\n",
    "    },\n",
    "    7: {\n",
    "        'primary': [\"Memory Access\", \"Knowledge Retrieval\", \"Historical Context\"],\n",
    "        'secondary': [\"Cache Management\", \"Index Optimization\", \"Query Planning\"],\n",
    "        'technical': [\"Vector Database\", \"Approximate Nearest Neighbor\", \"Inverted Index\"],\n",
    "        'metadata': NodeMetadata(52.1, 0.96, 0.04, 800, [6], \"memory_system\")\n",
    "    },\n",
    "    8: {\n",
    "        'primary': [\"Embedding Generation\", \"Semantic Representation\", \"Vector Space Mapping\"],\n",
    "        'secondary': [\"Dimensionality Reduction\", \"Feature Selection\", \"Normalization\"],\n",
    "        'technical': [\"Sentence-BERT\", \"Universal Sentence Encoder\", \"Contrastive Learning\"],\n",
    "        'metadata': NodeMetadata(89.7, 0.87, 0.13, 250, [4, 7], \"embedding_engine\")\n",
    "    },\n",
    "    9: {\n",
    "        'primary': [\"Error Recovery\", \"Fallback Processing\", \"Graceful Degradation\"],\n",
    "        'secondary': [\"Circuit Breaker\", \"Retry Logic\", \"Default Responses\"],\n",
    "        'technical': [\"Rule-based System\", \"Template Matching\", \"Statistical Fallback\"],\n",
    "        'metadata': NodeMetadata(12.4, 0.75, 0.25, 1500, [4], \"fallback_handler\")\n",
    "    },\n",
    "    10: {\n",
    "        'primary': [\"Content Summarization\", \"Information Distillation\", \"Key Point Extraction\"],\n",
    "        'secondary': [\"Abstractive Summary\", \"Extractive Summary\", \"Multi-document Summary\"],\n",
    "        'technical': [\"T5 Model\", \"BART\", \"Pointer-Generator Network\"],\n",
    "        'metadata': NodeMetadata(156.8, 0.83, 0.17, 150, [8], \"summarizer\")\n",
    "    },\n",
    "    11: {\n",
    "        'primary': [\"Response Generation\", \"Natural Language Generation\", \"Content Creation\"],\n",
    "        'secondary': [\"Style Transfer\", \"Tone Adjustment\", \"Personalization\"],\n",
    "        'technical': [\"GPT Architecture\", \"Beam Search\", \"Nucleus Sampling\"],\n",
    "        'metadata': NodeMetadata(234.5, 0.88, 0.12, 100, [10], \"generator\")\n",
    "    },\n",
    "    12: {\n",
    "        'primary': [\"Output Formatting\", \"Response Structuring\", \"Final Assembly\"],\n",
    "        'secondary': [\"Template Application\", \"Markup Generation\", \"Media Embedding\"],\n",
    "        'technical': [\"JSON Serialization\", \"HTML Generation\", \"Content Negotiation\"],\n",
    "        'metadata': NodeMetadata(18.7, 0.97, 0.03, 900, [11], \"formatter\")\n",
    "    },\n",
    "    13: {\n",
    "        'primary': [\"Quality Assessment\", \"Content Validation\", \"Fact Checking\"],\n",
    "        'secondary': [\"Hallucination Detection\", \"Bias Detection\", \"Toxicity Screening\"],\n",
    "        'technical': [\"Discriminator Networks\", \"Fact Verification API\", \"Bias Metrics\"],\n",
    "        'metadata': NodeMetadata(76.3, 0.86, 0.14, 350, [12, 16], \"quality_checker\")\n",
    "    },\n",
    "    14: {\n",
    "        'primary': [\"Post-processing\", \"Content Enhancement\", \"Final Polishing\"],\n",
    "        'secondary': [\"Grammar Correction\", \"Style Optimization\", \"Readability Enhancement\"],\n",
    "        'technical': [\"Language Tool\", \"Style Transfer Models\", \"Readability Metrics\"],\n",
    "        'metadata': NodeMetadata(42.9, 0.93, 0.07, 450, [13], \"post_processor\")\n",
    "    },\n",
    "    15: {\n",
    "        'primary': [\"Response Delivery\", \"Output Gateway\", \"User Interface\"],\n",
    "        'secondary': [\"Response Caching\", \"CDN Distribution\", \"Rate Limiting\"],\n",
    "        'technical': [\"HTTP Response\", \"WebSocket\", \"Server-Sent Events\"],\n",
    "        'metadata': NodeMetadata(8.1, 0.99, 0.01, 2000, [14], \"output_handler\")\n",
    "    },\n",
    "    16: {\n",
    "        'primary': [\"Validation Engine\", \"Compliance Checker\", \"Safety Filter\"],\n",
    "        'secondary': [\"Policy Enforcement\", \"Content Moderation\", \"Ethical Guidelines\"],\n",
    "        'technical': [\"Rule Engine\", \"ML Classifiers\", \"Blacklist Filtering\"],\n",
    "        'metadata': NodeMetadata(58.4, 0.95, 0.05, 400, [11], \"validator\")\n",
    "    },\n",
    "    17: {\n",
    "        'primary': [\"Feedback Collector\", \"Performance Monitor\", \"Learning System\"],\n",
    "        'secondary': [\"Usage Analytics\", \"Error Tracking\", \"Model Evaluation\"],\n",
    "        'technical': [\"Telemetry\", \"A/B Testing\", \"Online Learning\"],\n",
    "        'metadata': NodeMetadata(25.6, 0.90, 0.10, 700, [15], \"feedback_system\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# Apply concepts and metadata to nodes\n",
    "logger.info(\"Applying node concepts and metadata...\")\n",
    "for node_id, node_data in concepts.items():\n",
    "    G.nodes[node_id]['concepts'] = node_data['primary']\n",
    "    G.nodes[node_id]['secondary_concepts'] = node_data['secondary']\n",
    "    G.nodes[node_id]['technical_details'] = node_data['technical']\n",
    "    G.nodes[node_id]['metadata'] = node_data['metadata']\n",
    "\n",
    "# 3. Define multiple traversal paths for different scenarios\n",
    "traversal_paths = {\n",
    "    'primary_flow': [1, 2, 3, 4, 8, 10, 11, 12, 13, 14, 15],\n",
    "    'fast_track': [1, 5, 6, 10, 11, 12, 15],\n",
    "    'fallback_flow': [1, 2, 4, 9, 12, 15],\n",
    "    'validation_heavy': [1, 2, 3, 4, 8, 10, 11, 16, 13, 14, 15],\n",
    "    'feedback_loop': [1, 2, 3, 4, 8, 10, 11, 12, 13, 14, 15, 17, 1]\n",
    "}\n",
    "\n",
    "# 4. Initialize visualizer and perform analysis\n",
    "logger.info(\"Initializing graph visualizer...\")\n",
    "visualizer = GraphVisualizer()\n",
    "\n",
    "# 5. Simulate comprehensive filtered content for each path\n",
    "filtered_content = {\n",
    "    1: \"\"\"Entry Point Analysis:\n",
    "    - Received user query: \"What's the weather like in Nairobi today?\"\n",
    "    - Request authenticated and validated\n",
    "    - Input sanitized and normalized\n",
    "    - Routing to NLU pipeline initiated\"\"\",\n",
    "    \n",
    "    2: \"\"\"Intent Classification Results:\n",
    "    - Primary intent: WEATHER_QUERY (confidence: 0.96)\n",
    "    - Secondary intents: LOCATION_QUERY (0.34), TIME_QUERY (0.28)\n",
    "    - Language detected: English (confidence: 0.99)\n",
    "    - Sentiment: Neutral (0.02)\n",
    "    - No toxicity detected\"\"\",\n",
    "    \n",
    "    3: \"\"\"Named Entity Recognition:\n",
    "    - Location: \"Nairobi\" (CITY, confidence: 0.94)\n",
    "    - Time: \"today\" (DATE, confidence: 0.87)\n",
    "    - Linked entities: Nairobi -> Q3870 (Wikidata)\n",
    "    - Geographical coordinates: -1.2921°, 36.8219°\n",
    "    - Timezone: EAT (UTC+3)\"\"\",\n",
    "    \n",
    "    4: \"\"\"Semantic Reasoning Engine:\n",
    "    - Context: Weather information request for specific location and time\n",
    "    - Temporal resolution: Current date (2025-06-17)\n",
    "    - Spatial resolution: Nairobi metropolitan area\n",
    "    - Required data sources: Weather API, Location services\n",
    "    - Confidence in understanding: 0.91\"\"\",\n",
    "    \n",
    "    5: \"\"\"Query Expansion Results:\n",
    "    - Synonyms: [\"climate\", \"atmospheric conditions\", \"meteorological data\"]\n",
    "    - Related terms: [\"temperature\", \"humidity\", \"precipitation\", \"forecast\"]\n",
    "    - Location expansions: [\"Nairobi Kenya\", \"Nairobi East Africa\"]\n",
    "    - Temporal expansions: [\"current weather\", \"today's forecast\"]\"\"\",\n",
    "    \n",
    "    6: \"\"\"Context Aggregation:\n",
    "    - Combined NER results with intent classification\n",
    "    - Integrated temporal and spatial constraints\n",
    "    - Merged query expansions with original query\n",
    "    - Confidence weighted fusion applied\n",
    "    - Final context score: 0.89\"\"\",\n",
    "    \n",
    "    7: \"\"\"Memory System Access:\n",
    "    - Cache lookup for recent weather queries: HIT\n",
    "    - Historical weather patterns for Nairobi: Retrieved\n",
    "    - User preference data: No specific weather preferences found\n",
    "    - Previous similar queries: 3 matches in last 24h\n",
    "    - Memory access latency: 52ms\"\"\",\n",
    "    \n",
    "    8: \"\"\"Embedding Generation:\n",
    "    - Query embedding: 768-dimensional vector generated\n",
    "    - Semantic similarity to cached queries: 0.87\n",
    "    - Weather pattern embeddings: Computed for seasonal context\n",
    "    - Location embeddings: Enhanced with geographical features\n",
    "    - Embedding quality score: 0.91\"\"\",\n",
    "    \n",
    "    9: \"\"\"Fallback Processing (if triggered):\n",
    "    - Weather API unavailable fallback activated\n",
    "    - Default response template selected\n",
    "    - Historical weather averages retrieved\n",
    "    - Graceful degradation message prepared\n",
    "    - Fallback confidence: 0.75\"\"\",\n",
    "    \n",
    "    10: \"\"\"Content Summarization:\n",
    "    - Weather data sources identified and ranked\n",
    "    - Key information extracted: Temperature, conditions, forecast\n",
    "    - Redundant information filtered out\n",
    "    - Summary coherence score: 0.88\n",
    "    - Information density optimized\"\"\",\n",
    "    \n",
    "    11: \"\"\"Response Generation:\n",
    "    - Natural language response crafted\n",
    "    - Personalization applied based on location\n",
    "    - Conversational tone maintained\n",
    "    - Technical details simplified for general audience\n",
    "    - Generation quality score: 0.92\"\"\",\n",
    "    \n",
    "    12: \"\"\"Output Formatting:\n",
    "    - Response structured in user-friendly format\n",
    "    - Weather icons and formatting applied\n",
    "    - Metadata added for rich display\n",
    "    - Mobile-responsive formatting applied\n",
    "    - Format validation: PASSED\"\"\",\n",
    "    \n",
    "    13: \"\"\"Quality Assessment:\n",
    "    - Fact-checking weather data sources: VERIFIED\n",
    "    - Hallucination detection: No issues found\n",
    "    - Bias assessment: Geographic bias minimal\n",
    "    - Content safety: APPROVED\n",
    "    - Overall quality score: 0.89\"\"\",\n",
    "    \n",
    "    14: \"\"\"Post-processing Enhancement:\n",
    "    - Grammar and style optimization applied\n",
    "    - Readability score: 8.2/10 (Good)\n",
    "    - Tone consistency maintained\n",
    "    - Cultural appropriateness verified\n",
    "    - Final polish: COMPLETE\"\"\",\n",
    "    \n",
    "    15: \"\"\"Response Delivery:\n",
    "    - HTTP response prepared (200 OK)\n",
    "    - Content-Type: application/json\n",
    "    - Response size: 1.2KB\n",
    "    - Cache headers set for 15 minutes\n",
    "    - Delivery latency: 8ms\"\"\",\n",
    "    \n",
    "    16: \"\"\"Validation Results:\n",
    "    - Policy compliance: PASSED\n",
    "    - Content moderation: APPROVED\n",
    "    - Safety guidelines: COMPLIANT\n",
    "    - Ethical review: No concerns\n",
    "    - Validation confidence: 0.95\"\"\",\n",
    "    \n",
    "    17: \"\"\"Feedback Collection:\n",
    "    - User interaction logged\n",
    "    - Performance metrics recorded\n",
    "    - Model evaluation data captured\n",
    "    - A/B test variant: Control group\n",
    "    - Feedback loop: ACTIVE\"\"\"\n",
    "}\n",
    "\n",
    "# 6. Demonstrate different traversal scenarios\n",
    "for path_name, path in traversal_paths.items():\n",
    "    logger.info(f\"Analyzing {path_name} traversal path...\")\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"TRAVERSAL SCENARIO: {path_name.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Path: {' -> '.join(map(str, path))}\")\n",
    "    \n",
    "    # Calculate path metrics\n",
    "    total_latency = sum(G[path[i]][path[i+1]]['latency_ms'] for i in range(len(path)-1) if G.has_edge(path[i], path[i+1]))\n",
    "    avg_confidence = sum(concepts[node]['metadata'].confidence_score for node in path) / len(path)\n",
    "    \n",
    "    print(f\"Total latency: {total_latency}ms\")\n",
    "    print(f\"Average confidence: {avg_confidence:.3f}\")\n",
    "    print(f\"Path length: {len(path)} nodes\")\n",
    "    \n",
    "    # Visualize the traversal\n",
    "    print(f\"\\nVisualizing {path_name} traversal...\")\n",
    "    visualizer.visualize_traversal(G, path)\n",
    "    \n",
    "    # Print detailed content for key nodes in path\n",
    "    print(f\"\\nDetailed content for {path_name}:\")\n",
    "    key_nodes = path[::max(1, len(path)//5)]  # Sample every 5th node or so\n",
    "    for node in key_nodes:\n",
    "        if node in filtered_content:\n",
    "            print(f\"\\n--- NODE {node}: {concepts[node]['primary'][0]} ---\")\n",
    "            print(filtered_content[node])\n",
    "\n",
    "# 7. Export graph analysis results\n",
    "logger.info(\"Exporting graph analysis results...\")\n",
    "graph_stats = {\n",
    "    'total_nodes': G.number_of_nodes(),\n",
    "    'total_edges': G.number_of_edges(),\n",
    "    'average_degree': sum(dict(G.degree()).values()) / G.number_of_nodes(),\n",
    "    'is_connected': nx.is_connected(G),\n",
    "    'diameter': nx.diameter(G) if nx.is_connected(G) else 'N/A',\n",
    "    'clustering_coefficient': nx.average_clustering(G),\n",
    "    'node_types': {node_data['metadata'].node_type for node_data in concepts.values()}\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"GRAPH ANALYSIS SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "for key, value in graph_stats.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "logger.info(\"Complex graph prototype analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality = nx.degree_centrality(graph)\n",
    "sampled_nodes = sorted(degree_centrality, key=degree_centrality.get, reverse=True)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "56125596d14fe9c206f0f7e1_2025-06-07T06-49-21-437Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
